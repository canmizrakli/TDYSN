{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true,
      "authorship_tag": "ABX9TyOQmxMIwXMT0zvK11v8fCEB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/canmizrakli/TDSP.Net/blob/main/TDSP_YOLO_v2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# YOLO-based Task-driven Visual Saliency Prediction\n",
        "\n",
        "This notebook demonstrates a complete workflow for building a YOLO-based architecture for task-based visual saliency prediction.\n",
        "\n",
        "**Dataset Structure**  \n",
        "Your dataset (stored on Google Drive at `/content/drive/MyDrive/TDSP/Task-based-eye-fixation-dataset_1024x768`) should be organized as follows:\n",
        "\n",
        "\n",
        "\n",
        "**Task Definitions**  \n",
        "- **task1** â†’ free view  \n",
        "- **task2** â†’ count people  \n",
        "- **task3** â†’ detect the emotion  \n",
        "- **task4** â†’ identify the action\n",
        "\n",
        "The notebook includes:  \n",
        "- Paired data augmentation  \n",
        "- A YOLO-based backbone (pre-trained)  \n",
        "- A TaskEncoder using Sentence-BERT for rich task embeddings  \n",
        "- A saliency-specific loss function (KL Divergence + (1 - Correlation Coefficient))\n",
        "\n"
      ],
      "metadata": {
        "id": "A7GG_LwmgPIw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocessing"
      ],
      "metadata": {
        "id": "3rG2LzGKtahj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D3FqRtHIgRhN",
        "outputId": "03c44a67-3fd7-457e-e1dd-c6103ecfb77e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Install Dependencies\n",
        "!pip install --upgrade torch torchvision einops\n",
        "!pip install ultralytics"
      ],
      "metadata": {
        "collapsed": true,
        "id": "atkSTRsggTrO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Augmentation & Paired Transforms"
      ],
      "metadata": {
        "id": "u1nt_deHwyRv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import torchvision.transforms.functional as TF  # Use TF to avoid conflicts with torch.nn.functional\n",
        "\n",
        "# Paired random horizontal flip\n",
        "class PairedRandomHorizontalFlip:\n",
        "    def __init__(self, p=0.5):\n",
        "        self.p = p\n",
        "\n",
        "    def __call__(self, img, sal):\n",
        "        if random.random() < self.p:\n",
        "            img = TF.hflip(img)\n",
        "            sal = TF.hflip(sal)\n",
        "        return img, sal\n",
        "\n",
        "# Paired random rotation\n",
        "class PairedRandomRotation:\n",
        "    def __init__(self, degrees=10):\n",
        "        self.degrees = degrees\n",
        "\n",
        "    def __call__(self, img, sal):\n",
        "        angle = random.uniform(-self.degrees, self.degrees)\n",
        "        img = TF.rotate(img, angle)\n",
        "        sal = TF.rotate(sal, angle)\n",
        "        return img, sal"
      ],
      "metadata": {
        "id": "Jkc_8acKwzzH"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset Definition"
      ],
      "metadata": {
        "id": "24M8sb9Kggy8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import glob\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image\n",
        "import torchvision.transforms as T\n",
        "\n",
        "# Define task mapping from folder names to descriptive labels.\n",
        "task_mapping = {\n",
        "    \"task1\": \"free view\",\n",
        "    \"task2\": \"count people\",\n",
        "    \"task3\": \"detect the emotion\",\n",
        "    \"task4\": \"identify the action\"\n",
        "}\n",
        "\n",
        "class TaskSaliencyDataset(Dataset):\n",
        "    def __init__(self, data_root, task_mapping, transform=None, saliency_transform=None, paired_transforms=None):\n",
        "        \"\"\"\n",
        "        data_root: Root path of the dataset.\n",
        "        task_mapping: Dictionary mapping task folder names (e.g., \"task1\") to task descriptions.\n",
        "        transform: Transforms for stimuli images (e.g., resizing, ToTensor).\n",
        "        saliency_transform: Transforms for saliency maps.\n",
        "        paired_transforms: List of callables that apply the same random transform to both stimuli and saliency.\n",
        "        \"\"\"\n",
        "        self.data_root = data_root\n",
        "        self.task_mapping = task_mapping\n",
        "        self.transform = transform\n",
        "        self.saliency_transform = saliency_transform\n",
        "        self.paired_transforms = paired_transforms\n",
        "\n",
        "        self.tasks = list(task_mapping.keys())\n",
        "        self.samples = []\n",
        "\n",
        "        # Iterate over each task folder and gather samples\n",
        "        for task in self.tasks:\n",
        "            task_folder = os.path.join(data_root, task)\n",
        "            fdm_folder = os.path.join(task_folder, \"fdm\")\n",
        "            fdm_files = glob.glob(os.path.join(fdm_folder, \"*.png\"))\n",
        "            for fdm_file in fdm_files:\n",
        "                filename = os.path.basename(fdm_file)\n",
        "                base = os.path.splitext(filename)[0]\n",
        "                # Look for corresponding stimuli image (.jpg or .png)\n",
        "                stimuli_path_jpg = os.path.join(data_root, \"stimuli\", base + \".jpg\")\n",
        "                stimuli_path_png = os.path.join(data_root, \"stimuli\", base + \".png\")\n",
        "                if os.path.exists(stimuli_path_jpg):\n",
        "                    stimuli_path = stimuli_path_jpg\n",
        "                elif os.path.exists(stimuli_path_png):\n",
        "                    stimuli_path = stimuli_path_png\n",
        "                else:\n",
        "                    continue\n",
        "                self.samples.append((stimuli_path, fdm_file, task))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        stimuli_path, fdm_path, task = self.samples[idx]\n",
        "        stimuli_img = Image.open(stimuli_path).convert(\"RGB\")\n",
        "        fdm_img = Image.open(fdm_path).convert(\"L\")\n",
        "\n",
        "        if self.transform:\n",
        "            stimuli_img = self.transform(stimuli_img)\n",
        "        if self.saliency_transform:\n",
        "            fdm_img = self.saliency_transform(fdm_img)\n",
        "        else:\n",
        "            fdm_img = T.ToTensor()(fdm_img)\n",
        "\n",
        "        # Apply paired transforms if defined\n",
        "        if self.paired_transforms is not None:\n",
        "            for t in self.paired_transforms:\n",
        "                stimuli_img, fdm_img = t(stimuli_img, fdm_img)\n",
        "\n",
        "        task_description = self.task_mapping[task]\n",
        "\n",
        "        return {\n",
        "            \"stimuli\": stimuli_img,\n",
        "            \"fdm\": fdm_img,\n",
        "            \"task\": task,\n",
        "            \"task_description\": task_description\n",
        "        }\n",
        "\n",
        "# Define transforms\n",
        "input_transform = T.Compose([\n",
        "    T.Resize((384, 384)),\n",
        "    T.ToTensor(),\n",
        "])\n",
        "saliency_transform = T.Compose([\n",
        "    T.Resize((384, 384)),\n",
        "    T.ToTensor(),\n",
        "])\n",
        "paired_transforms = [\n",
        "    PairedRandomHorizontalFlip(p=0.5),\n",
        "    PairedRandomRotation(degrees=10)\n",
        "]\n",
        "\n",
        "# Path to your dataset (adjust the path if needed)\n",
        "DATA_PATH = \"/content/drive/MyDrive/TDSP/Task-based-eye-fixation-dataset_1024x768\"\n",
        "\n",
        "dataset = TaskSaliencyDataset(\n",
        "    data_root=DATA_PATH,\n",
        "    task_mapping=task_mapping,\n",
        "    transform=input_transform,\n",
        "    saliency_transform=saliency_transform,\n",
        "    paired_transforms=paired_transforms\n",
        ")\n",
        "dataloader = DataLoader(dataset, batch_size=4, shuffle=True, num_workers=0)\n",
        "\n",
        "print(\"Tasks found:\", dataset.tasks)\n",
        "print(\"Number of samples:\", len(dataset))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2PYNliOdgU3e",
        "outputId": "3a8978aa-63ff-4f8e-ebe4-c788c6e95aa2"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tasks found: ['task1', 'task2', 'task3', 'task4']\n",
            "Number of samples: 1968\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Components"
      ],
      "metadata": {
        "id": "S9MhzaNvglvi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Imports"
      ],
      "metadata": {
        "id": "xGl5b1RupvnT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from einops import rearrange\n",
        "from ultralytics import YOLO\n",
        "from sentence_transformers import SentenceTransformer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ijv3WTPovaL",
        "outputId": "8003e9e4-3e53-4f37-db1b-43747762332e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating new Ultralytics Settings v0.0.6 file âœ… \n",
            "View Ultralytics Settings with 'yolo settings' or at '/root/.config/Ultralytics/settings.json'\n",
            "Update Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###1. Pre-trained YOLO Backbone\n",
        "\n",
        "A YOLO (e.g., YOLOv5) model with its final detection layers removed, leaving only the feature extraction layers. This part extracts deep visual features from the input image."
      ],
      "metadata": {
        "id": "UCeCe-vnoxNn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 6.1 Pre-trained YOLO Backbone\n",
        "class YOLOBackbone(nn.Module):\n",
        "    def __init__(self, model_name=\"yolov5s.pt\"):\n",
        "        super().__init__()\n",
        "        # Load pre-trained YOLO model by specifying the weight file\n",
        "        self.yolo_model = YOLO(model_name)\n",
        "        # Retain only the feature extraction layers (adjust slicing as needed)\n",
        "        self.feature_extractor = self.yolo_model.model.model[:10]\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.feature_extractor(x)\n",
        "\n",
        "    def train(self, mode=True):\n",
        "        # Override train method to only set training mode on the feature extractor\n",
        "        self.feature_extractor.train(mode)\n",
        "        return self"
      ],
      "metadata": {
        "id": "vB-krZ47o1gA"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Feature Pyramid Network (FPN)\n",
        "\n",
        "A simple FPN-like module that reduces the channel dimension from (e.g.) 512 to 128. This helps unify the feature map so it can be fused with the task embedding."
      ],
      "metadata": {
        "id": "GA9_wTjKo2gv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 6.2 Simple FPN for multi-scale feature fusion\n",
        "class SimpleFPN(nn.Module):\n",
        "    def __init__(self, in_channels=512, out_channels=128):\n",
        "        super().__init__()\n",
        "        self.conv_out = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
        "\n",
        "    def forward(self, backbone_feats):\n",
        "        x = self.conv_out(backbone_feats)\n",
        "        return x"
      ],
      "metadata": {
        "id": "VyL_A3o6pGtJ"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Task Encoder\n",
        "\n",
        "Uses Sentence-BERT to convert a short text description (e.g., â€œcount peopleâ€) into a dense embedding (e.g., 64 dimensions). This embedding captures semantic information about the task."
      ],
      "metadata": {
        "id": "WswUT1edpHKC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 6.3 Task Encoder using pre-trained text embeddings (Sentence-BERT)\n",
        "class TaskEncoder(nn.Module):\n",
        "    def __init__(self, output_dim=64):\n",
        "        super().__init__()\n",
        "        self.text_encoder = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "        # Sentence-BERT outputs 384-dimensional embeddings; reduce to output_dim.\n",
        "        self.linear = nn.Linear(384, output_dim)\n",
        "\n",
        "    def forward(self, task_descriptions):\n",
        "        # Process a list of task description strings\n",
        "        embeddings = self.text_encoder.encode(task_descriptions, convert_to_tensor=True)\n",
        "        embeddings = self.linear(embeddings)\n",
        "        return F.relu(embeddings)"
      ],
      "metadata": {
        "id": "NjXrk9UppOO7"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###4. Transformer Fusion Module\n",
        "\n",
        "Merges the visual feature map with the task embedding. The visual features are flattened into tokens; the task embedding is added as an extra token; a transformer encoder processes them together. This allows the model to focus on task-relevant parts of the image."
      ],
      "metadata": {
        "id": "4Hl-k5y7pQIc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 6.4 Transformer Fusion Module\n",
        "class TransformerFusion(nn.Module):\n",
        "    def __init__(self, d_model=128, nhead=4, num_layers=1, task_embed_dim=64):\n",
        "        super().__init__()\n",
        "        self.query_proj = nn.Linear(task_embed_dim, d_model)\n",
        "        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead)\n",
        "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
        "\n",
        "    def forward(self, vision_feats, task_embed):\n",
        "        B, C, H, W = vision_feats.shape\n",
        "        vision_seq = rearrange(vision_feats, 'b c h w -> (h w) b c')\n",
        "        task_query = self.query_proj(task_embed)\n",
        "        task_query = rearrange(task_query, 'b d -> 1 b d')\n",
        "        fused_seq = torch.cat([task_query, vision_seq], dim=0)\n",
        "        encoded_seq = self.transformer_encoder(fused_seq)\n",
        "        encoded_vision = encoded_seq[1:, :, :]\n",
        "        encoded_vision = rearrange(encoded_vision, '(h w) b c -> b c h w', h=H, w=W)\n",
        "        return encoded_vision"
      ],
      "metadata": {
        "id": "H3JwOWhXpWa5"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###5. Saliency Decoder\n",
        "\n",
        "A small decoder (convolutional layers + batch normalization + ReLU) that converts the fused feature map into a single-channel saliency map. A final sigmoid squashes values into the [0,1] range."
      ],
      "metadata": {
        "id": "0MO8g0oHpWz_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 6.5 Saliency Decoder\n",
        "class SaliencyDecoder(nn.Module):\n",
        "    def __init__(self, in_channels=128):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels, 64, kernel_size=3, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.conv2 = nn.Conv2d(64, 1, kernel_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.bn1(self.conv1(x)))\n",
        "        x = self.conv2(x)\n",
        "        return torch.sigmoid(x)"
      ],
      "metadata": {
        "id": "IzOztQLcpeb1"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###6. The Complete Model: YOLOTaskSaliencyModel"
      ],
      "metadata": {
        "id": "UsUDo6OFpjlW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# 6.6 Complete Model: YOLOTaskSaliencyModel\n",
        "class YOLOTaskSaliencyModel(nn.Module):\n",
        "    def __init__(self, task_embed_dim=64, vision_dim=128, nhead=4, num_layers=1):\n",
        "        super().__init__()\n",
        "        self.backbone = YOLOBackbone(model_name=\"yolov5s.pt\")\n",
        "        self.fpn = SimpleFPN(in_channels=512, out_channels=128)  # Updated here\n",
        "        self.task_encoder = TaskEncoder(output_dim=task_embed_dim)\n",
        "        self.transformer_fusion = TransformerFusion(d_model=vision_dim, nhead=nhead,\n",
        "                                                    num_layers=num_layers, task_embed_dim=task_embed_dim)\n",
        "        self.saliency_decoder = SaliencyDecoder(in_channels=vision_dim)\n",
        "\n",
        "    def forward(self, images, task_descriptions):\n",
        "        feat = self.backbone(images)          # Output shape: [B, 512, H/?, W/?]\n",
        "        feat = self.fpn(feat)                 # Now shape becomes [B, 128, H/?, W/?]\n",
        "        task_embed = self.task_encoder(task_descriptions)  # [B, task_embed_dim]\n",
        "        fused_feat = self.transformer_fusion(feat, task_embed)  # [B, 128, H/?, W/?]\n",
        "        saliency_map = self.saliency_decoder(fused_feat)  # [B, 1, H/?, W/?]\n",
        "        saliency_map_upsampled = F.interpolate(\n",
        "            saliency_map,\n",
        "            size=(images.shape[2], images.shape[3]),  # match input's H, W\n",
        "            mode='bilinear',\n",
        "            align_corners=False\n",
        "        )\n",
        "        return saliency_map_upsampled"
      ],
      "metadata": {
        "id": "qszHw6OUgid2"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loss Function"
      ],
      "metadata": {
        "id": "PuHxlP11tobx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Saliency-Specific Loss Function\n",
        "\n",
        "A custom loss combining KL Divergence and (1 - Pearsonâ€™s Correlation Coefficient). Encourages the predicted map to match the ground-truth saliency distribution while being correlated with it spatially."
      ],
      "metadata": {
        "id": "zvioZoYwyyjT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SaliencyLoss(nn.Module):\n",
        "    def __init__(self, alpha=1.0, beta=1.0):\n",
        "        \"\"\"\n",
        "        alpha: Weight for the KL Divergence term.\n",
        "        beta: Weight for (1 - Pearson's Correlation Coefficient) term.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.alpha = alpha\n",
        "        self.beta = beta\n",
        "\n",
        "    def forward(self, pred, gt):\n",
        "        # Ensure shape is [B, H, W]\n",
        "        if len(pred.shape) == 4:\n",
        "            pred = pred[:, 0]\n",
        "        if len(gt.shape) == 4:\n",
        "            gt = gt[:, 0]\n",
        "\n",
        "        B, H, W = pred.shape\n",
        "        pred = pred.reshape(B, -1)\n",
        "        gt = gt.reshape(B, -1)\n",
        "        eps = 1e-12\n",
        "        pred_norm = pred / (pred.sum(dim=1, keepdim=True) + eps)\n",
        "        gt_norm = gt / (gt.sum(dim=1, keepdim=True) + eps)\n",
        "\n",
        "        # KL Divergence: sum(gt * log(gt/pred))\n",
        "        kl = (gt_norm * torch.log((gt_norm + eps) / (pred_norm + eps))).sum(dim=1).mean()\n",
        "\n",
        "        # Pearson's Correlation Coefficient (CC)\n",
        "        pred_mean = pred.mean(dim=1, keepdim=True)\n",
        "        gt_mean = gt.mean(dim=1, keepdim=True)\n",
        "        numerator = ((pred - pred_mean) * (gt - gt_mean)).sum(dim=1)\n",
        "        denominator = torch.sqrt(((pred - pred_mean)**2).sum(dim=1) * ((gt - gt_mean)**2).sum(dim=1)) + eps\n",
        "        cc = (numerator / denominator).mean()\n",
        "\n",
        "        loss = self.alpha * kl + self.beta * (1 - cc)\n",
        "        return loss"
      ],
      "metadata": {
        "id": "KWnuKmBVyzQi"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training"
      ],
      "metadata": {
        "id": "6HqtHz9Ytqlz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "lr = 1e-4\n",
        "num_epochs = 5\n",
        "\n",
        "model = YOLOTaskSaliencyModel().to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "criterion = SaliencyLoss(alpha=1.0, beta=1.0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uWvrNrYptBhU",
        "outputId": "e65f8d23-1090-4e98-a30d-446ce5d1a276"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PRO TIP ðŸ’¡ Replace 'model=yolov5s.pt' with new 'model=yolov5su.pt'.\n",
            "YOLOv5 'u' models are trained with https://github.com/ultralytics/ultralytics and feature improved performance vs standard YOLOv5 models trained with https://github.com/ultralytics/yolov5.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for batch_idx, batch_data in enumerate(dataloader):\n",
        "        images = batch_data[\"stimuli\"].to(device)   # [B, 3, H, W]\n",
        "        fdm = batch_data[\"fdm\"].to(device)           # [B, 1, H, W]\n",
        "        # Task descriptions are returned as a list of strings\n",
        "        task_descs = batch_data[\"task_description\"]\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        pred_saliency = model(images, task_descs)    # Forward pass using task descriptions\n",
        "        loss = criterion(pred_saliency, fdm)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        if (batch_idx + 1) % 10 == 0:\n",
        "            print(f\"Epoch [{epoch+1}/{num_epochs}], Batch [{batch_idx+1}/{len(dataloader)}] Loss: {loss.item():.4f}\", flush=True)\n",
        "    epoch_loss = running_loss / len(dataloader)\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}] Average Loss: {epoch_loss:.4f}\", flush=True)\n"
      ],
      "metadata": {
        "id": "VMLl6KIagnqR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5467602f-c8c6-4690-c267-e5aa476b693c",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PRO TIP ðŸ’¡ Replace 'model=yolov5s.pt' with new 'model=yolov5su.pt'.\n",
            "YOLOv5 'u' models are trained with https://github.com/ultralytics/ultralytics and feature improved performance vs standard YOLOv5 models trained with https://github.com/ultralytics/yolov5.\n",
            "\n",
            "Epoch [1/5], Batch [10/492] Loss: 2.6052\n",
            "Epoch [1/5], Batch [20/492] Loss: 2.5195\n",
            "Epoch [1/5], Batch [30/492] Loss: 2.4999\n",
            "Epoch [1/5], Batch [40/492] Loss: 2.1462\n",
            "Epoch [1/5], Batch [50/492] Loss: 2.0817\n",
            "Epoch [1/5], Batch [60/492] Loss: 2.0553\n",
            "Epoch [1/5], Batch [70/492] Loss: 2.5043\n",
            "Epoch [1/5], Batch [80/492] Loss: 2.2365\n",
            "Epoch [1/5], Batch [90/492] Loss: 2.5806\n",
            "Epoch [1/5], Batch [100/492] Loss: 2.4580\n",
            "Epoch [1/5], Batch [110/492] Loss: 2.1273\n",
            "Epoch [1/5], Batch [120/492] Loss: 2.2526\n",
            "Epoch [1/5], Batch [130/492] Loss: 2.2204\n",
            "Epoch [1/5], Batch [140/492] Loss: 2.4205\n",
            "Epoch [1/5], Batch [150/492] Loss: 2.0845\n",
            "Epoch [1/5], Batch [160/492] Loss: 2.0704\n",
            "Epoch [1/5], Batch [170/492] Loss: 2.1187\n",
            "Epoch [1/5], Batch [180/492] Loss: 2.2197\n",
            "Epoch [1/5], Batch [190/492] Loss: 2.1385\n",
            "Epoch [1/5], Batch [200/492] Loss: 2.0144\n",
            "Epoch [1/5], Batch [210/492] Loss: 1.9937\n",
            "Epoch [1/5], Batch [220/492] Loss: 2.2272\n",
            "Epoch [1/5], Batch [230/492] Loss: 2.0415\n",
            "Epoch [1/5], Batch [240/492] Loss: 2.1651\n",
            "Epoch [1/5], Batch [250/492] Loss: 1.9553\n",
            "Epoch [1/5], Batch [260/492] Loss: 2.0650\n",
            "Epoch [1/5], Batch [270/492] Loss: 2.3350\n",
            "Epoch [1/5], Batch [280/492] Loss: 2.4001\n",
            "Epoch [1/5], Batch [290/492] Loss: 2.1497\n",
            "Epoch [1/5], Batch [300/492] Loss: 2.1573\n",
            "Epoch [1/5], Batch [310/492] Loss: 1.8519\n",
            "Epoch [1/5], Batch [320/492] Loss: 2.1710\n",
            "Epoch [1/5], Batch [330/492] Loss: 2.2671\n",
            "Epoch [1/5], Batch [340/492] Loss: 1.9169\n",
            "Epoch [1/5], Batch [350/492] Loss: 1.9879\n",
            "Epoch [1/5], Batch [360/492] Loss: 2.1026\n",
            "Epoch [1/5], Batch [370/492] Loss: 2.2572\n",
            "Epoch [1/5], Batch [380/492] Loss: 1.7033\n",
            "Epoch [1/5], Batch [390/492] Loss: 1.9967\n",
            "Epoch [1/5], Batch [400/492] Loss: 1.7387\n",
            "Epoch [1/5], Batch [410/492] Loss: 1.9938\n",
            "Epoch [1/5], Batch [420/492] Loss: 2.1260\n",
            "Epoch [1/5], Batch [430/492] Loss: 1.9949\n",
            "Epoch [1/5], Batch [440/492] Loss: 2.0622\n",
            "Epoch [1/5], Batch [450/492] Loss: 1.8415\n",
            "Epoch [1/5], Batch [460/492] Loss: 1.7889\n",
            "Epoch [1/5], Batch [470/492] Loss: 1.6653\n",
            "Epoch [1/5], Batch [480/492] Loss: 1.7270\n",
            "Epoch [1/5], Batch [490/492] Loss: 2.0020\n",
            "Epoch [1/5] Average Loss: 2.1703\n",
            "Epoch [2/5], Batch [10/492] Loss: 1.9086\n",
            "Epoch [2/5], Batch [20/492] Loss: 2.1518\n",
            "Epoch [2/5], Batch [30/492] Loss: 1.8022\n",
            "Epoch [2/5], Batch [40/492] Loss: 2.1934\n",
            "Epoch [2/5], Batch [50/492] Loss: 1.9724\n",
            "Epoch [2/5], Batch [60/492] Loss: 1.9737\n",
            "Epoch [2/5], Batch [70/492] Loss: 1.9381\n",
            "Epoch [2/5], Batch [80/492] Loss: 1.9491\n",
            "Epoch [2/5], Batch [90/492] Loss: 1.8631\n",
            "Epoch [2/5], Batch [100/492] Loss: 2.0463\n",
            "Epoch [2/5], Batch [110/492] Loss: 1.8230\n",
            "Epoch [2/5], Batch [120/492] Loss: 1.7975\n",
            "Epoch [2/5], Batch [130/492] Loss: 1.8587\n",
            "Epoch [2/5], Batch [140/492] Loss: 2.0088\n",
            "Epoch [2/5], Batch [150/492] Loss: 2.0154\n",
            "Epoch [2/5], Batch [160/492] Loss: 1.7089\n",
            "Epoch [2/5], Batch [170/492] Loss: 1.9175\n",
            "Epoch [2/5], Batch [180/492] Loss: 1.6612\n",
            "Epoch [2/5], Batch [190/492] Loss: 1.7874\n",
            "Epoch [2/5], Batch [200/492] Loss: 1.5617\n",
            "Epoch [2/5], Batch [210/492] Loss: 1.5403\n",
            "Epoch [2/5], Batch [220/492] Loss: 1.6229\n",
            "Epoch [2/5], Batch [230/492] Loss: 1.7530\n",
            "Epoch [2/5], Batch [240/492] Loss: 1.4822\n",
            "Epoch [2/5], Batch [250/492] Loss: 1.7894\n",
            "Epoch [2/5], Batch [260/492] Loss: 1.9991\n",
            "Epoch [2/5], Batch [270/492] Loss: 1.6962\n",
            "Epoch [2/5], Batch [280/492] Loss: 1.9417\n",
            "Epoch [2/5], Batch [290/492] Loss: 1.8726\n",
            "Epoch [2/5], Batch [300/492] Loss: 1.7561\n",
            "Epoch [2/5], Batch [310/492] Loss: 1.8099\n",
            "Epoch [2/5], Batch [320/492] Loss: 1.4894\n",
            "Epoch [2/5], Batch [330/492] Loss: 2.1227\n",
            "Epoch [2/5], Batch [340/492] Loss: 1.5905\n",
            "Epoch [2/5], Batch [350/492] Loss: 1.8105\n",
            "Epoch [2/5], Batch [360/492] Loss: 1.9554\n",
            "Epoch [2/5], Batch [370/492] Loss: 1.7881\n",
            "Epoch [2/5], Batch [380/492] Loss: 1.5456\n",
            "Epoch [2/5], Batch [390/492] Loss: 1.5828\n",
            "Epoch [2/5], Batch [400/492] Loss: 1.7697\n",
            "Epoch [2/5], Batch [410/492] Loss: 1.5435\n",
            "Epoch [2/5], Batch [420/492] Loss: 1.4670\n",
            "Epoch [2/5], Batch [430/492] Loss: 1.8006\n",
            "Epoch [2/5], Batch [440/492] Loss: 2.1004\n",
            "Epoch [2/5], Batch [450/492] Loss: 1.4919\n",
            "Epoch [2/5], Batch [460/492] Loss: 1.8087\n",
            "Epoch [2/5], Batch [470/492] Loss: 2.0412\n",
            "Epoch [2/5], Batch [480/492] Loss: 1.6216\n",
            "Epoch [2/5], Batch [490/492] Loss: 1.8600\n",
            "Epoch [2/5] Average Loss: 1.8079\n",
            "Epoch [3/5], Batch [10/492] Loss: 1.5757\n",
            "Epoch [3/5], Batch [20/492] Loss: 1.8361\n",
            "Epoch [3/5], Batch [30/492] Loss: 1.4312\n",
            "Epoch [3/5], Batch [40/492] Loss: 1.7399\n",
            "Epoch [3/5], Batch [50/492] Loss: 1.6377\n",
            "Epoch [3/5], Batch [60/492] Loss: 1.6095\n",
            "Epoch [3/5], Batch [70/492] Loss: 1.4015\n",
            "Epoch [3/5], Batch [80/492] Loss: 1.4253\n",
            "Epoch [3/5], Batch [90/492] Loss: 1.9864\n",
            "Epoch [3/5], Batch [100/492] Loss: 1.8108\n",
            "Epoch [3/5], Batch [110/492] Loss: 2.1923\n",
            "Epoch [3/5], Batch [120/492] Loss: 1.6787\n",
            "Epoch [3/5], Batch [130/492] Loss: 2.0228\n",
            "Epoch [3/5], Batch [140/492] Loss: 1.5802\n",
            "Epoch [3/5], Batch [150/492] Loss: 1.3982\n",
            "Epoch [3/5], Batch [160/492] Loss: 1.6545\n",
            "Epoch [3/5], Batch [170/492] Loss: 1.6357\n",
            "Epoch [3/5], Batch [180/492] Loss: 1.8471\n",
            "Epoch [3/5], Batch [190/492] Loss: 1.5380\n",
            "Epoch [3/5], Batch [200/492] Loss: 1.7262\n",
            "Epoch [3/5], Batch [210/492] Loss: 1.6097\n",
            "Epoch [3/5], Batch [220/492] Loss: 1.7891\n",
            "Epoch [3/5], Batch [230/492] Loss: 1.6466\n",
            "Epoch [3/5], Batch [240/492] Loss: 1.3982\n",
            "Epoch [3/5], Batch [250/492] Loss: 1.5555\n",
            "Epoch [3/5], Batch [260/492] Loss: 1.6555\n",
            "Epoch [3/5], Batch [270/492] Loss: 1.6816\n",
            "Epoch [3/5], Batch [280/492] Loss: 1.7723\n",
            "Epoch [3/5], Batch [290/492] Loss: 1.7260\n",
            "Epoch [3/5], Batch [300/492] Loss: 1.8148\n",
            "Epoch [3/5], Batch [310/492] Loss: 1.5704\n",
            "Epoch [3/5], Batch [320/492] Loss: 1.6366\n",
            "Epoch [3/5], Batch [330/492] Loss: 1.6427\n",
            "Epoch [3/5], Batch [340/492] Loss: 1.7903\n",
            "Epoch [3/5], Batch [350/492] Loss: 2.3174\n",
            "Epoch [3/5], Batch [360/492] Loss: 2.0599\n",
            "Epoch [3/5], Batch [370/492] Loss: 1.2868\n",
            "Epoch [3/5], Batch [380/492] Loss: 2.0440\n",
            "Epoch [3/5], Batch [390/492] Loss: 2.0045\n",
            "Epoch [3/5], Batch [400/492] Loss: 1.5596\n",
            "Epoch [3/5], Batch [410/492] Loss: 1.6742\n",
            "Epoch [3/5], Batch [420/492] Loss: 1.5156\n",
            "Epoch [3/5], Batch [430/492] Loss: 1.8142\n",
            "Epoch [3/5], Batch [440/492] Loss: 1.4983\n",
            "Epoch [3/5], Batch [450/492] Loss: 2.1865\n",
            "Epoch [3/5], Batch [460/492] Loss: 1.3538\n",
            "Epoch [3/5], Batch [470/492] Loss: 1.6134\n",
            "Epoch [3/5], Batch [480/492] Loss: 1.5257\n",
            "Epoch [3/5], Batch [490/492] Loss: 1.8541\n",
            "Epoch [3/5] Average Loss: 1.6457\n",
            "Epoch [4/5], Batch [10/492] Loss: 1.5934\n",
            "Epoch [4/5], Batch [20/492] Loss: 1.6338\n",
            "Epoch [4/5], Batch [30/492] Loss: 1.8065\n",
            "Epoch [4/5], Batch [40/492] Loss: 1.8063\n",
            "Epoch [4/5], Batch [50/492] Loss: 1.3154\n",
            "Epoch [4/5], Batch [60/492] Loss: 1.4487\n",
            "Epoch [4/5], Batch [70/492] Loss: 1.5340\n",
            "Epoch [4/5], Batch [80/492] Loss: 1.6510\n",
            "Epoch [4/5], Batch [90/492] Loss: 1.9990\n",
            "Epoch [4/5], Batch [100/492] Loss: 1.7577\n",
            "Epoch [4/5], Batch [110/492] Loss: 1.3040\n",
            "Epoch [4/5], Batch [120/492] Loss: 1.8657\n",
            "Epoch [4/5], Batch [130/492] Loss: 1.7396\n",
            "Epoch [4/5], Batch [140/492] Loss: 1.3905\n",
            "Epoch [4/5], Batch [150/492] Loss: 1.6571\n",
            "Epoch [4/5], Batch [160/492] Loss: 1.5504\n",
            "Epoch [4/5], Batch [170/492] Loss: 1.4393\n",
            "Epoch [4/5], Batch [180/492] Loss: 1.9911\n",
            "Epoch [4/5], Batch [190/492] Loss: 1.6656\n",
            "Epoch [4/5], Batch [200/492] Loss: 1.5838\n",
            "Epoch [4/5], Batch [210/492] Loss: 1.4963\n",
            "Epoch [4/5], Batch [220/492] Loss: 1.4024\n",
            "Epoch [4/5], Batch [230/492] Loss: 1.4268\n",
            "Epoch [4/5], Batch [240/492] Loss: 1.4731\n",
            "Epoch [4/5], Batch [250/492] Loss: 1.9228\n",
            "Epoch [4/5], Batch [260/492] Loss: 1.7714\n",
            "Epoch [4/5], Batch [270/492] Loss: 1.3957\n",
            "Epoch [4/5], Batch [280/492] Loss: 1.5999\n",
            "Epoch [4/5], Batch [290/492] Loss: 1.7492\n",
            "Epoch [4/5], Batch [300/492] Loss: 1.8698\n",
            "Epoch [4/5], Batch [310/492] Loss: 1.4140\n",
            "Epoch [4/5], Batch [320/492] Loss: 1.4094\n",
            "Epoch [4/5], Batch [330/492] Loss: 1.5284\n",
            "Epoch [4/5], Batch [340/492] Loss: 1.6553\n",
            "Epoch [4/5], Batch [350/492] Loss: 2.0356\n",
            "Epoch [4/5], Batch [360/492] Loss: 2.0696\n",
            "Epoch [4/5], Batch [370/492] Loss: 1.8945\n",
            "Epoch [4/5], Batch [380/492] Loss: 1.7350\n",
            "Epoch [4/5], Batch [390/492] Loss: 1.7104\n",
            "Epoch [4/5], Batch [400/492] Loss: 1.8491\n",
            "Epoch [4/5], Batch [410/492] Loss: 1.7577\n",
            "Epoch [4/5], Batch [420/492] Loss: 1.7455\n",
            "Epoch [4/5], Batch [430/492] Loss: 1.3740\n",
            "Epoch [4/5], Batch [440/492] Loss: 1.6930\n",
            "Epoch [4/5], Batch [450/492] Loss: 1.9224\n",
            "Epoch [4/5], Batch [460/492] Loss: 1.5305\n",
            "Epoch [4/5], Batch [470/492] Loss: 1.5362\n",
            "Epoch [4/5], Batch [480/492] Loss: 1.5896\n",
            "Epoch [4/5], Batch [490/492] Loss: 1.7379\n",
            "Epoch [4/5] Average Loss: 1.5874\n",
            "Epoch [5/5], Batch [10/492] Loss: 1.4201\n",
            "Epoch [5/5], Batch [20/492] Loss: 1.5733\n",
            "Epoch [5/5], Batch [30/492] Loss: 1.6309\n",
            "Epoch [5/5], Batch [40/492] Loss: 1.5303\n",
            "Epoch [5/5], Batch [50/492] Loss: 1.2620\n",
            "Epoch [5/5], Batch [60/492] Loss: 1.6319\n",
            "Epoch [5/5], Batch [70/492] Loss: 1.6207\n",
            "Epoch [5/5], Batch [80/492] Loss: 1.5205\n",
            "Epoch [5/5], Batch [90/492] Loss: 1.6063\n",
            "Epoch [5/5], Batch [100/492] Loss: 1.7487\n",
            "Epoch [5/5], Batch [110/492] Loss: 1.4563\n",
            "Epoch [5/5], Batch [120/492] Loss: 1.4054\n",
            "Epoch [5/5], Batch [130/492] Loss: 1.4223\n",
            "Epoch [5/5], Batch [140/492] Loss: 1.6973\n",
            "Epoch [5/5], Batch [150/492] Loss: 1.7588\n",
            "Epoch [5/5], Batch [160/492] Loss: 1.3644\n",
            "Epoch [5/5], Batch [170/492] Loss: 1.3914\n",
            "Epoch [5/5], Batch [180/492] Loss: 1.8822\n",
            "Epoch [5/5], Batch [190/492] Loss: 1.3061\n",
            "Epoch [5/5], Batch [200/492] Loss: 1.3492\n",
            "Epoch [5/5], Batch [210/492] Loss: 1.6158\n",
            "Epoch [5/5], Batch [220/492] Loss: 1.6048\n",
            "Epoch [5/5], Batch [230/492] Loss: 1.6393\n",
            "Epoch [5/5], Batch [240/492] Loss: 1.6717\n",
            "Epoch [5/5], Batch [250/492] Loss: 1.5742\n",
            "Epoch [5/5], Batch [260/492] Loss: 1.4548\n",
            "Epoch [5/5], Batch [270/492] Loss: 1.6450\n",
            "Epoch [5/5], Batch [280/492] Loss: 1.5400\n",
            "Epoch [5/5], Batch [290/492] Loss: 1.9339\n",
            "Epoch [5/5], Batch [300/492] Loss: 1.6160\n",
            "Epoch [5/5], Batch [310/492] Loss: 1.4678\n",
            "Epoch [5/5], Batch [320/492] Loss: 1.8351\n",
            "Epoch [5/5], Batch [330/492] Loss: 1.4656\n",
            "Epoch [5/5], Batch [340/492] Loss: 1.4515\n",
            "Epoch [5/5], Batch [350/492] Loss: 1.6188\n",
            "Epoch [5/5], Batch [360/492] Loss: 1.5337\n",
            "Epoch [5/5], Batch [370/492] Loss: 1.3844\n",
            "Epoch [5/5], Batch [380/492] Loss: 1.5603\n",
            "Epoch [5/5], Batch [390/492] Loss: 1.4525\n",
            "Epoch [5/5], Batch [400/492] Loss: 1.6420\n",
            "Epoch [5/5], Batch [410/492] Loss: 1.6785\n",
            "Epoch [5/5], Batch [420/492] Loss: 1.7481\n",
            "Epoch [5/5], Batch [430/492] Loss: 1.5219\n",
            "Epoch [5/5], Batch [440/492] Loss: 1.7406\n",
            "Epoch [5/5], Batch [450/492] Loss: 1.4011\n",
            "Epoch [5/5], Batch [460/492] Loss: 1.5200\n",
            "Epoch [5/5], Batch [470/492] Loss: 1.5657\n",
            "Epoch [5/5], Batch [480/492] Loss: 1.3965\n",
            "Epoch [5/5], Batch [490/492] Loss: 1.6337\n",
            "Epoch [5/5] Average Loss: 1.5480\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the model state dictionary to the specified path\n",
        "save_path = \"/content/drive/MyDrive/TDSP/yolo_task_saliency_model.pth\"\n",
        "torch.save(model.state_dict(), save_path)\n",
        "print(f\"Model saved to {save_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LnB1NffpUF-s",
        "outputId": "c3b02186-e03c-4b23-d8a4-6934bf0efaf7"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model saved to /content/drive/MyDrive/TDSP/yolo_task_saliency_model.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# to later call the model\n",
        "model.load_state_dict(torch.load(save_path))\n",
        "model.eval()  # Set the model to evaluation model"
      ],
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NrkhdwxbUHjY",
        "outputId": "0f2785d4-7858-4173-fec6-0b9920729170"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "YOLOTaskSaliencyModel(\n",
              "  (backbone): YOLOBackbone(\n",
              "    (yolo_model): YOLO(\n",
              "      (model): DetectionModel(\n",
              "        (model): Sequential(\n",
              "          (0): Conv(\n",
              "            (conv): Conv2d(3, 32, kernel_size=(6, 6), stride=(2, 2), padding=(2, 2), bias=False)\n",
              "            (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "            (act): SiLU(inplace=True)\n",
              "          )\n",
              "          (1): Conv(\n",
              "            (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "            (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "            (act): SiLU(inplace=True)\n",
              "          )\n",
              "          (2): C3(\n",
              "            (cv1): Conv(\n",
              "              (conv): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "              (act): SiLU(inplace=True)\n",
              "            )\n",
              "            (cv2): Conv(\n",
              "              (conv): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "              (act): SiLU(inplace=True)\n",
              "            )\n",
              "            (cv3): Conv(\n",
              "              (conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "              (act): SiLU(inplace=True)\n",
              "            )\n",
              "            (m): Sequential(\n",
              "              (0): Bottleneck(\n",
              "                (cv1): Conv(\n",
              "                  (conv): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "                  (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "                  (act): SiLU(inplace=True)\n",
              "                )\n",
              "                (cv2): Conv(\n",
              "                  (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "                  (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "                  (act): SiLU(inplace=True)\n",
              "                )\n",
              "              )\n",
              "            )\n",
              "          )\n",
              "          (3): Conv(\n",
              "            (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "            (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "            (act): SiLU(inplace=True)\n",
              "          )\n",
              "          (4): C3(\n",
              "            (cv1): Conv(\n",
              "              (conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "              (act): SiLU(inplace=True)\n",
              "            )\n",
              "            (cv2): Conv(\n",
              "              (conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "              (act): SiLU(inplace=True)\n",
              "            )\n",
              "            (cv3): Conv(\n",
              "              (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "              (act): SiLU(inplace=True)\n",
              "            )\n",
              "            (m): Sequential(\n",
              "              (0): Bottleneck(\n",
              "                (cv1): Conv(\n",
              "                  (conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "                  (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "                  (act): SiLU(inplace=True)\n",
              "                )\n",
              "                (cv2): Conv(\n",
              "                  (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "                  (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "                  (act): SiLU(inplace=True)\n",
              "                )\n",
              "              )\n",
              "              (1): Bottleneck(\n",
              "                (cv1): Conv(\n",
              "                  (conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "                  (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "                  (act): SiLU(inplace=True)\n",
              "                )\n",
              "                (cv2): Conv(\n",
              "                  (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "                  (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "                  (act): SiLU(inplace=True)\n",
              "                )\n",
              "              )\n",
              "            )\n",
              "          )\n",
              "          (5): Conv(\n",
              "            (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "            (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "            (act): SiLU(inplace=True)\n",
              "          )\n",
              "          (6): C3(\n",
              "            (cv1): Conv(\n",
              "              (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "              (act): SiLU(inplace=True)\n",
              "            )\n",
              "            (cv2): Conv(\n",
              "              (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "              (act): SiLU(inplace=True)\n",
              "            )\n",
              "            (cv3): Conv(\n",
              "              (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "              (act): SiLU(inplace=True)\n",
              "            )\n",
              "            (m): Sequential(\n",
              "              (0): Bottleneck(\n",
              "                (cv1): Conv(\n",
              "                  (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "                  (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "                  (act): SiLU(inplace=True)\n",
              "                )\n",
              "                (cv2): Conv(\n",
              "                  (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "                  (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "                  (act): SiLU(inplace=True)\n",
              "                )\n",
              "              )\n",
              "              (1): Bottleneck(\n",
              "                (cv1): Conv(\n",
              "                  (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "                  (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "                  (act): SiLU(inplace=True)\n",
              "                )\n",
              "                (cv2): Conv(\n",
              "                  (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "                  (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "                  (act): SiLU(inplace=True)\n",
              "                )\n",
              "              )\n",
              "              (2): Bottleneck(\n",
              "                (cv1): Conv(\n",
              "                  (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "                  (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "                  (act): SiLU(inplace=True)\n",
              "                )\n",
              "                (cv2): Conv(\n",
              "                  (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "                  (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "                  (act): SiLU(inplace=True)\n",
              "                )\n",
              "              )\n",
              "            )\n",
              "          )\n",
              "          (7): Conv(\n",
              "            (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "            (bn): BatchNorm2d(512, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "            (act): SiLU(inplace=True)\n",
              "          )\n",
              "          (8): C3(\n",
              "            (cv1): Conv(\n",
              "              (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "              (act): SiLU(inplace=True)\n",
              "            )\n",
              "            (cv2): Conv(\n",
              "              (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "              (act): SiLU(inplace=True)\n",
              "            )\n",
              "            (cv3): Conv(\n",
              "              (conv): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (bn): BatchNorm2d(512, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "              (act): SiLU(inplace=True)\n",
              "            )\n",
              "            (m): Sequential(\n",
              "              (0): Bottleneck(\n",
              "                (cv1): Conv(\n",
              "                  (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "                  (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "                  (act): SiLU(inplace=True)\n",
              "                )\n",
              "                (cv2): Conv(\n",
              "                  (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "                  (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "                  (act): SiLU(inplace=True)\n",
              "                )\n",
              "              )\n",
              "            )\n",
              "          )\n",
              "          (9): SPPF(\n",
              "            (cv1): Conv(\n",
              "              (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "              (act): SiLU(inplace=True)\n",
              "            )\n",
              "            (cv2): Conv(\n",
              "              (conv): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (bn): BatchNorm2d(512, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "              (act): SiLU(inplace=True)\n",
              "            )\n",
              "            (m): MaxPool2d(kernel_size=5, stride=1, padding=2, dilation=1, ceil_mode=False)\n",
              "          )\n",
              "          (10): Conv(\n",
              "            (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "            (act): SiLU(inplace=True)\n",
              "          )\n",
              "          (11): Upsample(scale_factor=2.0, mode='nearest')\n",
              "          (12): Concat()\n",
              "          (13): C3(\n",
              "            (cv1): Conv(\n",
              "              (conv): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "              (act): SiLU(inplace=True)\n",
              "            )\n",
              "            (cv2): Conv(\n",
              "              (conv): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "              (act): SiLU(inplace=True)\n",
              "            )\n",
              "            (cv3): Conv(\n",
              "              (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "              (act): SiLU(inplace=True)\n",
              "            )\n",
              "            (m): Sequential(\n",
              "              (0): Bottleneck(\n",
              "                (cv1): Conv(\n",
              "                  (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "                  (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "                  (act): SiLU(inplace=True)\n",
              "                )\n",
              "                (cv2): Conv(\n",
              "                  (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "                  (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "                  (act): SiLU(inplace=True)\n",
              "                )\n",
              "              )\n",
              "            )\n",
              "          )\n",
              "          (14): Conv(\n",
              "            (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "            (act): SiLU(inplace=True)\n",
              "          )\n",
              "          (15): Upsample(scale_factor=2.0, mode='nearest')\n",
              "          (16): Concat()\n",
              "          (17): C3(\n",
              "            (cv1): Conv(\n",
              "              (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "              (act): SiLU(inplace=True)\n",
              "            )\n",
              "            (cv2): Conv(\n",
              "              (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "              (act): SiLU(inplace=True)\n",
              "            )\n",
              "            (cv3): Conv(\n",
              "              (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "              (act): SiLU(inplace=True)\n",
              "            )\n",
              "            (m): Sequential(\n",
              "              (0): Bottleneck(\n",
              "                (cv1): Conv(\n",
              "                  (conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "                  (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "                  (act): SiLU(inplace=True)\n",
              "                )\n",
              "                (cv2): Conv(\n",
              "                  (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "                  (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "                  (act): SiLU(inplace=True)\n",
              "                )\n",
              "              )\n",
              "            )\n",
              "          )\n",
              "          (18): Conv(\n",
              "            (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "            (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "            (act): SiLU(inplace=True)\n",
              "          )\n",
              "          (19): Concat()\n",
              "          (20): C3(\n",
              "            (cv1): Conv(\n",
              "              (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "              (act): SiLU(inplace=True)\n",
              "            )\n",
              "            (cv2): Conv(\n",
              "              (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "              (act): SiLU(inplace=True)\n",
              "            )\n",
              "            (cv3): Conv(\n",
              "              (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "              (act): SiLU(inplace=True)\n",
              "            )\n",
              "            (m): Sequential(\n",
              "              (0): Bottleneck(\n",
              "                (cv1): Conv(\n",
              "                  (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "                  (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "                  (act): SiLU(inplace=True)\n",
              "                )\n",
              "                (cv2): Conv(\n",
              "                  (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "                  (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "                  (act): SiLU(inplace=True)\n",
              "                )\n",
              "              )\n",
              "            )\n",
              "          )\n",
              "          (21): Conv(\n",
              "            (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "            (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "            (act): SiLU(inplace=True)\n",
              "          )\n",
              "          (22): Concat()\n",
              "          (23): C3(\n",
              "            (cv1): Conv(\n",
              "              (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "              (act): SiLU(inplace=True)\n",
              "            )\n",
              "            (cv2): Conv(\n",
              "              (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "              (act): SiLU(inplace=True)\n",
              "            )\n",
              "            (cv3): Conv(\n",
              "              (conv): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (bn): BatchNorm2d(512, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "              (act): SiLU(inplace=True)\n",
              "            )\n",
              "            (m): Sequential(\n",
              "              (0): Bottleneck(\n",
              "                (cv1): Conv(\n",
              "                  (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "                  (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "                  (act): SiLU(inplace=True)\n",
              "                )\n",
              "                (cv2): Conv(\n",
              "                  (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "                  (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "                  (act): SiLU(inplace=True)\n",
              "                )\n",
              "              )\n",
              "            )\n",
              "          )\n",
              "          (24): Detect(\n",
              "            (cv2): ModuleList(\n",
              "              (0): Sequential(\n",
              "                (0): Conv(\n",
              "                  (conv): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "                  (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "                  (act): SiLU(inplace=True)\n",
              "                )\n",
              "                (1): Conv(\n",
              "                  (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "                  (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "                  (act): SiLU(inplace=True)\n",
              "                )\n",
              "                (2): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
              "              )\n",
              "              (1): Sequential(\n",
              "                (0): Conv(\n",
              "                  (conv): Conv2d(256, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "                  (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "                  (act): SiLU(inplace=True)\n",
              "                )\n",
              "                (1): Conv(\n",
              "                  (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "                  (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "                  (act): SiLU(inplace=True)\n",
              "                )\n",
              "                (2): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
              "              )\n",
              "              (2): Sequential(\n",
              "                (0): Conv(\n",
              "                  (conv): Conv2d(512, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "                  (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "                  (act): SiLU(inplace=True)\n",
              "                )\n",
              "                (1): Conv(\n",
              "                  (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "                  (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "                  (act): SiLU(inplace=True)\n",
              "                )\n",
              "                (2): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
              "              )\n",
              "            )\n",
              "            (cv3): ModuleList(\n",
              "              (0): Sequential(\n",
              "                (0): Conv(\n",
              "                  (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "                  (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "                  (act): SiLU(inplace=True)\n",
              "                )\n",
              "                (1): Conv(\n",
              "                  (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "                  (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "                  (act): SiLU(inplace=True)\n",
              "                )\n",
              "                (2): Conv2d(128, 80, kernel_size=(1, 1), stride=(1, 1))\n",
              "              )\n",
              "              (1): Sequential(\n",
              "                (0): Conv(\n",
              "                  (conv): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "                  (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "                  (act): SiLU(inplace=True)\n",
              "                )\n",
              "                (1): Conv(\n",
              "                  (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "                  (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "                  (act): SiLU(inplace=True)\n",
              "                )\n",
              "                (2): Conv2d(128, 80, kernel_size=(1, 1), stride=(1, 1))\n",
              "              )\n",
              "              (2): Sequential(\n",
              "                (0): Conv(\n",
              "                  (conv): Conv2d(512, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "                  (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "                  (act): SiLU(inplace=True)\n",
              "                )\n",
              "                (1): Conv(\n",
              "                  (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "                  (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "                  (act): SiLU(inplace=True)\n",
              "                )\n",
              "                (2): Conv2d(128, 80, kernel_size=(1, 1), stride=(1, 1))\n",
              "              )\n",
              "            )\n",
              "            (dfl): DFL(\n",
              "              (conv): Conv2d(16, 1, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (feature_extractor): Sequential(\n",
              "      (0): Conv(\n",
              "        (conv): Conv2d(3, 32, kernel_size=(6, 6), stride=(2, 2), padding=(2, 2), bias=False)\n",
              "        (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "        (act): SiLU(inplace=True)\n",
              "      )\n",
              "      (1): Conv(\n",
              "        (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "        (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "        (act): SiLU(inplace=True)\n",
              "      )\n",
              "      (2): C3(\n",
              "        (cv1): Conv(\n",
              "          (conv): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "          (act): SiLU(inplace=True)\n",
              "        )\n",
              "        (cv2): Conv(\n",
              "          (conv): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "          (act): SiLU(inplace=True)\n",
              "        )\n",
              "        (cv3): Conv(\n",
              "          (conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "          (act): SiLU(inplace=True)\n",
              "        )\n",
              "        (m): Sequential(\n",
              "          (0): Bottleneck(\n",
              "            (cv1): Conv(\n",
              "              (conv): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "              (act): SiLU(inplace=True)\n",
              "            )\n",
              "            (cv2): Conv(\n",
              "              (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "              (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "              (act): SiLU(inplace=True)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (3): Conv(\n",
              "        (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "        (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "        (act): SiLU(inplace=True)\n",
              "      )\n",
              "      (4): C3(\n",
              "        (cv1): Conv(\n",
              "          (conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "          (act): SiLU(inplace=True)\n",
              "        )\n",
              "        (cv2): Conv(\n",
              "          (conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "          (act): SiLU(inplace=True)\n",
              "        )\n",
              "        (cv3): Conv(\n",
              "          (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "          (act): SiLU(inplace=True)\n",
              "        )\n",
              "        (m): Sequential(\n",
              "          (0): Bottleneck(\n",
              "            (cv1): Conv(\n",
              "              (conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "              (act): SiLU(inplace=True)\n",
              "            )\n",
              "            (cv2): Conv(\n",
              "              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "              (act): SiLU(inplace=True)\n",
              "            )\n",
              "          )\n",
              "          (1): Bottleneck(\n",
              "            (cv1): Conv(\n",
              "              (conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "              (act): SiLU(inplace=True)\n",
              "            )\n",
              "            (cv2): Conv(\n",
              "              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "              (act): SiLU(inplace=True)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (5): Conv(\n",
              "        (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "        (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "        (act): SiLU(inplace=True)\n",
              "      )\n",
              "      (6): C3(\n",
              "        (cv1): Conv(\n",
              "          (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "          (act): SiLU(inplace=True)\n",
              "        )\n",
              "        (cv2): Conv(\n",
              "          (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "          (act): SiLU(inplace=True)\n",
              "        )\n",
              "        (cv3): Conv(\n",
              "          (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "          (act): SiLU(inplace=True)\n",
              "        )\n",
              "        (m): Sequential(\n",
              "          (0): Bottleneck(\n",
              "            (cv1): Conv(\n",
              "              (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "              (act): SiLU(inplace=True)\n",
              "            )\n",
              "            (cv2): Conv(\n",
              "              (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "              (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "              (act): SiLU(inplace=True)\n",
              "            )\n",
              "          )\n",
              "          (1): Bottleneck(\n",
              "            (cv1): Conv(\n",
              "              (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "              (act): SiLU(inplace=True)\n",
              "            )\n",
              "            (cv2): Conv(\n",
              "              (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "              (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "              (act): SiLU(inplace=True)\n",
              "            )\n",
              "          )\n",
              "          (2): Bottleneck(\n",
              "            (cv1): Conv(\n",
              "              (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "              (act): SiLU(inplace=True)\n",
              "            )\n",
              "            (cv2): Conv(\n",
              "              (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "              (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "              (act): SiLU(inplace=True)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (7): Conv(\n",
              "        (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "        (bn): BatchNorm2d(512, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "        (act): SiLU(inplace=True)\n",
              "      )\n",
              "      (8): C3(\n",
              "        (cv1): Conv(\n",
              "          (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "          (act): SiLU(inplace=True)\n",
              "        )\n",
              "        (cv2): Conv(\n",
              "          (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "          (act): SiLU(inplace=True)\n",
              "        )\n",
              "        (cv3): Conv(\n",
              "          (conv): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn): BatchNorm2d(512, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "          (act): SiLU(inplace=True)\n",
              "        )\n",
              "        (m): Sequential(\n",
              "          (0): Bottleneck(\n",
              "            (cv1): Conv(\n",
              "              (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "              (act): SiLU(inplace=True)\n",
              "            )\n",
              "            (cv2): Conv(\n",
              "              (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "              (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "              (act): SiLU(inplace=True)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (9): SPPF(\n",
              "        (cv1): Conv(\n",
              "          (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "          (act): SiLU(inplace=True)\n",
              "        )\n",
              "        (cv2): Conv(\n",
              "          (conv): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn): BatchNorm2d(512, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "          (act): SiLU(inplace=True)\n",
              "        )\n",
              "        (m): MaxPool2d(kernel_size=5, stride=1, padding=2, dilation=1, ceil_mode=False)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (fpn): SimpleFPN(\n",
              "    (conv_out): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))\n",
              "  )\n",
              "  (task_encoder): TaskEncoder(\n",
              "    (text_encoder): SentenceTransformer(\n",
              "      (0): Transformer({'max_seq_length': 256, 'do_lower_case': False}) with Transformer model: BertModel \n",
              "      (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n",
              "      (2): Normalize()\n",
              "    )\n",
              "    (linear): Linear(in_features=384, out_features=64, bias=True)\n",
              "  )\n",
              "  (transformer_fusion): TransformerFusion(\n",
              "    (query_proj): Linear(in_features=64, out_features=128, bias=True)\n",
              "    (transformer_encoder): TransformerEncoder(\n",
              "      (layers): ModuleList(\n",
              "        (0): TransformerEncoderLayer(\n",
              "          (self_attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
              "          )\n",
              "          (linear1): Linear(in_features=128, out_features=2048, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "          (linear2): Linear(in_features=2048, out_features=128, bias=True)\n",
              "          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "          (dropout1): Dropout(p=0.1, inplace=False)\n",
              "          (dropout2): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (saliency_decoder): SaliencyDecoder(\n",
              "    (conv1): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (conv2): Conv2d(64, 1, kernel_size=(1, 1), stride=(1, 1))\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inference\n"
      ],
      "metadata": {
        "id": "BF5nOw9Nttft"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluation & Visualization"
      ],
      "metadata": {
        "id": "G-xTBSP8y-t1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    batch_data = next(iter(dataloader))\n",
        "    images = batch_data[\"stimuli\"].to(device)\n",
        "    task_descs = batch_data[\"task_description\"]\n",
        "    pred_saliency = model(images, task_descs)\n",
        "\n",
        "pred_saliency_np = pred_saliency.cpu().numpy()\n",
        "fdm_np = batch_data[\"fdm\"].cpu().numpy()\n",
        "\n",
        "num_samples = pred_saliency_np.shape[0]\n",
        "fig, axes = plt.subplots(2, num_samples, figsize=(15, 6))\n",
        "for i in range(num_samples):\n",
        "    axes[0, i].imshow(pred_saliency_np[i, 0], cmap='jet')\n",
        "    axes[0, i].set_title(\"Predicted\")\n",
        "    axes[0, i].axis('off')\n",
        "\n",
        "    axes[1, i].imshow(fdm_np[i, 0], cmap='jet')\n",
        "    axes[1, i].set_title(\"Ground Truth\")\n",
        "    axes[1, i].axis('off')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "y7y3bJt_knRw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluate model performance"
      ],
      "metadata": {
        "id": "buITmXRFRjPp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "def correlation_coefficient(pred, gt):\n",
        "    \"\"\"\n",
        "    Computes Pearson's Correlation Coefficient between two saliency maps.\n",
        "    pred, gt: torch.Tensor of shape [H, W], assumed float in [0,1].\n",
        "    Returns a scalar tensor (higher is better).\n",
        "    \"\"\"\n",
        "    # Flatten to 1D\n",
        "    pred_flat = pred.view(-1)\n",
        "    gt_flat = gt.view(-1)\n",
        "\n",
        "    pred_mean = pred_flat.mean()\n",
        "    gt_mean = gt_flat.mean()\n",
        "\n",
        "    numerator = ((pred_flat - pred_mean) * (gt_flat - gt_mean)).sum()\n",
        "    denominator = torch.sqrt(((pred_flat - pred_mean)**2).sum() * ((gt_flat - gt_mean)**2).sum()) + 1e-12\n",
        "\n",
        "    return numerator / denominator\n",
        "\n",
        "def kl_divergence(pred, gt):\n",
        "    \"\"\"\n",
        "    Computes KL Divergence between two saliency maps treated as probability distributions.\n",
        "    pred, gt: torch.Tensor of shape [H, W], assumed float in [0,1].\n",
        "    Returns a scalar tensor (lower is better).\n",
        "    \"\"\"\n",
        "    eps = 1e-12\n",
        "    # Normalize so each sums to 1\n",
        "    pred_norm = pred / (pred.sum() + eps)\n",
        "    gt_norm = gt / (gt.sum() + eps)\n",
        "\n",
        "    return (gt_norm * torch.log((gt_norm + eps) / (pred_norm + eps))).sum()\n"
      ],
      "metadata": {
        "id": "sW_Vcg8qqBf8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_saliency_model(model, dataloader, device):\n",
        "    model.eval()\n",
        "    cc_scores = []\n",
        "    kld_scores = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_data in dataloader:\n",
        "            images = batch_data[\"stimuli\"].to(device)  # [B, 3, H, W]\n",
        "            fdm = batch_data[\"fdm\"].to(device)         # [B, 1, H, W]\n",
        "            task_descs = batch_data[\"task_description\"]\n",
        "\n",
        "            # 1. Forward pass\n",
        "            pred_saliency = model(images, task_descs)  # [B, 1, H, W]\n",
        "\n",
        "            # 2. For each item in the batch\n",
        "            for i in range(images.size(0)):\n",
        "                pred_map = pred_saliency[i, 0]  # shape [H, W]\n",
        "                gt_map = fdm[i, 0]             # shape [H, W]\n",
        "\n",
        "                # 3. Compute metrics\n",
        "                cc = correlation_coefficient(pred_map, gt_map)\n",
        "                kld = kl_divergence(pred_map, gt_map)\n",
        "\n",
        "                cc_scores.append(cc.item())\n",
        "                kld_scores.append(kld.item())\n",
        "\n",
        "    # 4. Compute averages\n",
        "    avg_cc = sum(cc_scores) / len(cc_scores) if cc_scores else 0\n",
        "    avg_kld = sum(kld_scores) / len(kld_scores) if kld_scores else 0\n",
        "\n",
        "    print(f\"Average CC: {avg_cc:.4f}\")\n",
        "    print(f\"Average KLD: {avg_kld:.4f}\")\n",
        "    return avg_cc, avg_kld\n",
        "\n",
        "# Example usage:\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "avg_cc, avg_kld = evaluate_saliency_model(model, dataloader, device)\n"
      ],
      "metadata": {
        "id": "u2DYZVgBRlzP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}